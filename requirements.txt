# Memory Transformer Requirements
# For cloud training on A100s

# Core
torch>=2.1.0
numpy>=1.24.0
pyyaml>=6.0
tqdm>=4.65.0

# Tokenizers and datasets
tokenizers>=0.15.0
datasets>=2.14.0
transformers>=4.35.0

# Distributed training
accelerate>=0.24.0

# Logging and experiment tracking
wandb>=0.16.0
tensorboard>=2.14.0

# Evaluation
scikit-learn>=1.3.0
rouge-score>=0.1.2

# Utilities
einops>=0.7.0
safetensors>=0.4.0

# Optional: Flash Attention for faster training (A100 only)
# pip install flash-attn --no-build-isolation
