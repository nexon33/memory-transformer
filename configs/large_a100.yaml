# Large Memory Transformer for A100 Training
# ~500M parameters - Full scale experiment

# Model dimensions
d_model: 768
n_heads: 12
n_layers: 16
d_ff: 3072

# Vocabulary and sequence
vocab_size: 32000  # BPE tokenizer
max_seq_len: 2048

# Memory parameters
snapshot_interval: 4
memory_size: 512   # 512 memory slots for long context
d_memory: 768      # Full snapshot dimension
d_compressed: 192  # 4x compression
n_retrieval_hops: 3

# Memory variant
use_compressed: false
use_both: true     # Use hybrid for best results

# Regularization
dropout: 0.1
attention_dropout: 0.1
memory_dropout: 0.05

# Training
gradient_checkpointing: true  # Enable for large model

# Initialization
init_std: 0.02
