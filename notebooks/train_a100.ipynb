{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-Augmented Transformer Training (A100)\n",
    "\n",
    "**Quick start for Vertex AI Workbench or Colab Enterprise**\n",
    "\n",
    "This notebook trains the Memory-Augmented Transformer on A100 GPUs.\n",
    "\n",
    "## Setup\n",
    "1. Create A100 instance in Vertex AI Workbench\n",
    "2. Clone this repo\n",
    "3. Run cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q transformers datasets accelerate wandb tokenizers einops safetensors pyyaml tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo (if not already done)\n",
    "import os\n",
    "if not os.path.exists('memory_transformer'):\n",
    "    # Replace with your repo URL\n",
    "    !git clone https://github.com/YOUR_USERNAME/memory-transformer.git\n",
    "    %cd memory-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to wandb (optional but recommended)\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify setup\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"BF16 support: {torch.cuda.is_bf16_supported()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Options\n",
    "\n",
    "Choose your experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Quick validation (1 hour, ~$4)\n",
    "# Train small model to verify everything works\n",
    "!python scripts/train_cloud.py \\\n",
    "    --model-config configs/tiny_full.yaml \\\n",
    "    --batch-size 32 \\\n",
    "    --max-steps 5000 \\\n",
    "    --dataset c4 \\\n",
    "    --output-dir outputs/quick-test \\\n",
    "    --run-name \"quick-validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Medium model training (4-6 hours, ~$20)\n",
    "# 180M parameter model on C4\n",
    "!python scripts/train_cloud.py \\\n",
    "    --model-config configs/medium_a100.yaml \\\n",
    "    --batch-size 32 \\\n",
    "    --max-steps 50000 \\\n",
    "    --dataset c4 \\\n",
    "    --output-dir outputs/medium-c4 \\\n",
    "    --run-name \"mat-180m-c4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3: Large model training (10-15 hours, ~$50)\n",
    "# 500M parameter model\n",
    "!python scripts/train_cloud.py \\\n",
    "    --model-config configs/large_a100.yaml \\\n",
    "    --batch-size 16 \\\n",
    "    --max-steps 100000 \\\n",
    "    --dataset c4 \\\n",
    "    --output-dir outputs/large-c4 \\\n",
    "    --run-name \"mat-500m-c4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 4: Hyperparameter sweep (parallel, ~$100)\n",
    "# Run multiple experiments to find optimal config\n",
    "!wandb sweep configs/sweep_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU Training (8x A100)\n",
    "\n",
    "If you have access to 8x A100 (~$30/hr), use distributed training for 10x speedup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8x A100 distributed training\n",
    "!accelerate launch --num_processes=8 scripts/train_distributed.py \\\n",
    "    --model-config configs/large_a100.yaml \\\n",
    "    --batch-size 8 \\\n",
    "    --max-steps 50000 \\\n",
    "    --run-name \"mat-500m-8gpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained model\n",
    "!python scripts/evaluate.py \\\n",
    "    --checkpoint outputs/medium-c4/checkpoint-best.pt \\\n",
    "    --benchmark recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text from trained model\n",
    "!python scripts/generate.py \\\n",
    "    --checkpoint outputs/medium-c4/checkpoint-best.pt \\\n",
    "    --prompt \"The Memory-Augmented Transformer works by\" \\\n",
    "    --max-tokens 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to GCS\n",
    "\n",
    "Save checkpoints to Google Cloud Storage to persist after VM shutdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload checkpoints to GCS\n",
    "BUCKET = \"your-bucket-name\"  # Change this\n",
    "!gsutil -m cp -r outputs/* gs://{BUCKET}/mat-checkpoints/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "premium"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
